{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d636aa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting intent_generator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile intent_generator.py\n",
    "'''\n",
    "Import\n",
    "'''\n",
    "import darshan\n",
    "from enum import Enum\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6188422a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to intent_generator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a intent_generator.py\n",
    "'''\n",
    "Data Structures\n",
    "'''\n",
    "class SharingPattern(Enum):\n",
    "    INDEPENDENT=0\n",
    "    COLLECTIVE=1\n",
    "    OTHER=2\n",
    "class FileMode(Enum):\n",
    "    WRITE_ONLY=0\n",
    "    READ_ONLY=1\n",
    "    READ_WRITE=2\n",
    "    APPEND=3\n",
    "class AccessPatternType(Enum):\n",
    "    WRITE_ONLY = 0\n",
    "    READ_ONLY = 1\n",
    "    RAW = 2\n",
    "    OTHER = 3\n",
    "class MultiSessionIO:\n",
    "    open_timestamp: tuple() = (0,0)\n",
    "    close_timestamp: tuple() = (0,0)\n",
    "    read_timestamp: tuple() = (0,0)\n",
    "    write_timestamp: tuple() = (0,0)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self.json())\n",
    "\n",
    "    def json(self):\n",
    "        return {\n",
    "            'open_timestamp': [self.open_timestamp[0], self.open_timestamp[1]],\n",
    "            'close_timestamp': [self.close_timestamp[0], self.close_timestamp[1]],\n",
    "            'read_timestamp': [self.read_timestamp[0], self.read_timestamp[1]],\n",
    "            'write_timestamp': [self.write_timestamp[0], self.write_timestamp[1]],\n",
    "        }\n",
    "\n",
    "class DatasetIOIntents:\n",
    "    #metadata\n",
    "    filename = None\n",
    "    dataset_name = None\n",
    "    ndims = 1\n",
    "    # primary\n",
    "    session_io: MultiSessionIO = MultiSessionIO()\n",
    "    type: AccessPatternType = AccessPatternType.OTHER\n",
    "    top_accessed_segments = {}\n",
    "    transfer_size_dist = {}\n",
    "    process_sharing = []\n",
    "    fs_size = 0\n",
    "    sharing_pattern:SharingPattern = SharingPattern.OTHER\n",
    "    # secondary\n",
    "    mode: FileMode = FileMode.READ_WRITE\n",
    "    def __repr__(self):\n",
    "        return str(self.json())\n",
    "\n",
    "    def json(self):\n",
    "        return {\n",
    "            'filename': self.filename,\n",
    "            'dataset_name': self.dataset_name,\n",
    "            'ndims': self.ndims,\n",
    "            'session_io': self.session_io.json(),\n",
    "            'type': self.type.value,\n",
    "            'top_accessed_segments': self.top_accessed_segments,\n",
    "            'transfer_size_dist': self.transfer_size_dist,\n",
    "            'process_sharing': self.process_sharing,\n",
    "            'fs_size': self.fs_size,\n",
    "            'sharing_pattern': self.sharing_pattern.value,\n",
    "            'mode': self.mode.value,\n",
    "        }\n",
    "\n",
    "class FileIOIntents:\n",
    "    #metadata\n",
    "    filename = None\n",
    "    # primary\n",
    "    session_io: MultiSessionIO = MultiSessionIO()\n",
    "    mode: FileMode = FileMode.READ_WRITE\n",
    "    fs_size = 0\n",
    "    sharing_pattern:SharingPattern = SharingPattern.OTHER\n",
    "    # secondary\n",
    "    ap_distribution = {}\n",
    "    top_accessed_segments = {}\n",
    "    transfer_size_dist = {}\n",
    "    process_sharing = []\n",
    "    ds_size_dist = {}\n",
    "    def __repr__(self):\n",
    "        return str(self.json())\n",
    "\n",
    "    def json(self):\n",
    "        return {\n",
    "            'session_io': self.session_io.json(),\n",
    "            'mode': self.mode.value,\n",
    "            'process_sharing': self.process_sharing,\n",
    "            'fs_size': self.fs_size,\n",
    "            'sharing_pattern': self.sharing_pattern.value,\n",
    "            'ap_distribution': self.ap_distribution,\n",
    "            'top_accessed_segments': self.top_accessed_segments,\n",
    "            'transfer_size_dist': self.transfer_size_dist,\n",
    "            'process_sharing': self.process_sharing,\n",
    "            'ds_size_dist': self.ds_size_dist,\n",
    "        }\n",
    "class Intents:\n",
    "    def __init__(self):\n",
    "        self.files = {}\n",
    "        self.datasets = {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.json())\n",
    "\n",
    "    def json(self):\n",
    "        return {\n",
    "            'files': self.files,\n",
    "            'datasets': self.datasets\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "01d34adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to intent_generator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a intent_generator.py\n",
    "'''\n",
    "Constants\n",
    "'''\n",
    "KB = 1024\n",
    "MB = 1024 * 1024\n",
    "GB = 1024 * 1024 * 1024\n",
    "AVAIL_NODE_MEMORY_BYTES = 200 * GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f92f998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to intent_generator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a intent_generator.py\n",
    "''' encoder '''\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if hasattr(obj, 'json'):\n",
    "            return obj.json()\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a3c10187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to intent_generator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a intent_generator.py\n",
    "'''\n",
    "Main Class\n",
    "'''\n",
    "class IntentGenerator:\n",
    "    def __init__(self, base_path, darshan_logs, property_json, workflow, data_dirs):\n",
    "        self.base_path = base_path\n",
    "        self.darshan_logs = darshan_logs\n",
    "        self.property_json = property_json\n",
    "        self.workflow = workflow\n",
    "        self.app = {}\n",
    "        self.data_dirs = data_dirs.split(\":\")\n",
    "\n",
    "    def parse_apps(self):\n",
    "        folder = f\"{self.base_path}/{self.darshan_logs}/{self.workflow}\"\n",
    "        for file in os.listdir(folder):\n",
    "            if file.endswith(\".darshan\"):\n",
    "                app_name = \"_\".join(file.split(\"_\")[1:-3])\n",
    "                self.app[app_name] = {'path': os.path.join(folder, file),\n",
    "                                      'report': None,\n",
    "                                      'relevant_ids': [],\n",
    "                                      'name_to_id_map': {},\n",
    "                                      'num_processes': 0,\n",
    "                                      'file_agg': {},\n",
    "                                      'h5f_df_c': None,\n",
    "                                      'h5d_df_c': None,\n",
    "                                      'h5f_df_fc': None,\n",
    "                                      'h5d_df_fc': None,\n",
    "                                      'configuration': Intents()\n",
    "                                      }\n",
    "        # print(self.app)\n",
    "        return self.app\n",
    "\n",
    "    def load_apps(self):\n",
    "        for app_name, value in self.app.items():\n",
    "            self.load_darshan_log(app_name)\n",
    "            if self.found_hdf5:\n",
    "                self.read_dataset(app_name)\n",
    "                self.read_file(app_name)\n",
    "        return self.app\n",
    "\n",
    "    def write_configurations(self):\n",
    "        json_files = []\n",
    "        for app_name, value in self.app.items():\n",
    "            if self.found_hdf5:\n",
    "                json_object = json.dumps(self.app[app_name]['configuration'], cls=NpEncoder, indent=2)\n",
    "                folder = f\"{self.base_path}/{self.property_json}/{self.workflow}\"\n",
    "                if not os.path.exists(folder):\n",
    "                    Path(folder).mkdir(parents=True)\n",
    "                json_filename = f\"{folder}/{app_name}.json\"\n",
    "                json_files.append(json_filename)\n",
    "                with open(json_filename, \"w\") as outfile:\n",
    "                    outfile.write(json_object)\n",
    "        return json_files\n",
    "\n",
    "    def load_darshan_log(self, app_name):\n",
    "        report = darshan.DarshanReport(self.app[app_name]['path'], read_all=True)\n",
    "        self.app[app_name]['num_processes'] = report.data['metadata']['job']['nprocs']\n",
    "\n",
    "        for key, value in report.data['name_records'].items():\n",
    "            for data_dir in self.data_dirs:\n",
    "                #print(data_dir, value)\n",
    "                if data_dir in value:\n",
    "                    self.app[app_name]['relevant_ids'].append(key)\n",
    "                    self.app[app_name]['name_to_id_map'][value] = key                \n",
    "        print(report.modules.keys())\n",
    "        self.found_hdf5 = True\n",
    "        if \"H5F\" not in report.modules:\n",
    "            print(f\"No HDF5 Module found for {self.workflow}\")\n",
    "            self.found_hdf5 = False\n",
    "        if self.found_hdf5:\n",
    "            report.mod_read_all_records('H5F')\n",
    "            report.mod_read_all_records('H5D')\n",
    "            h5f_df_c = report.records['H5F'].to_df()['counters']\n",
    "            h5f_df_fc = report.records['H5F'].to_df()['fcounters']\n",
    "            h5d_df_c = report.records['H5D'].to_df()['counters']\n",
    "            h5d_df_fc = report.records['H5D'].to_df()['fcounters']\n",
    "            self.app[app_name]['h5f_df_c'] = h5f_df_c[h5f_df_c['id'].isin(self.app[app_name]['relevant_ids'])]\n",
    "            self.app[app_name]['h5d_df_c'] = h5d_df_c[h5d_df_c['id'].isin(self.app[app_name]['relevant_ids'])]\n",
    "            self.app[app_name]['h5f_df_fc'] = h5f_df_fc[h5f_df_fc['id'].isin(self.app[app_name]['relevant_ids'])]\n",
    "            self.app[app_name]['h5d_df_fc'] = h5d_df_fc[h5d_df_fc['id'].isin(self.app[app_name]['relevant_ids'])]\n",
    "        self.app[app_name]['report'] = report\n",
    "        return report\n",
    "    \n",
    "    def read_dataset(self, app_name, initial=True):\n",
    "        h5f_df_c = self.app[app_name]['h5f_df_c']\n",
    "        h5d_df_c = self.app[app_name]['h5d_df_c']\n",
    "        h5f_df_fc = self.app[app_name]['h5f_df_fc']\n",
    "        h5d_df_fc = self.app[app_name]['h5d_df_fc']\n",
    "        report = self.app[app_name]['report']\n",
    "        file_agg = {}\n",
    "        #print(h5d_df_c)\n",
    "        for ind in h5d_df_c.index:\n",
    "            ds_id = h5d_df_c['id'][ind]\n",
    "            dataset_fqn = report.data['name_records'][ds_id]\n",
    "            #print(dataset_fqn)\n",
    "            dset_split_fqn = dataset_fqn.split(\":\")\n",
    "            \n",
    "            dataset_intents = DatasetIOIntents()\n",
    "            dataset_intents.filename = dset_split_fqn[0]\n",
    "            dataset_intents.dataset_name = dataset_fqn\n",
    "            file_id = self.app[app_name]['name_to_id_map'][dataset_intents.filename]\n",
    "            if file_id not in file_agg:\n",
    "                file_agg[file_id] = {\n",
    "                    'fs_size': 0,\n",
    "                    'mode': {str(FileMode.READ_ONLY.value): 0,\n",
    "                            str(FileMode.WRITE_ONLY.value): 0,\n",
    "                            str(FileMode.READ_WRITE.value): 0,\n",
    "                            str(FileMode.APPEND.value): 0},\n",
    "                    'ap_distribution': {str(AccessPatternType.READ_ONLY.value): 0,\n",
    "                                        str(AccessPatternType.WRITE_ONLY.value): 0,\n",
    "                                        str(AccessPatternType.RAW.value): 0,\n",
    "                                        str(AccessPatternType.OTHER.value): 0},\n",
    "                    'top_accessed_segments': {},\n",
    "                    'transfer_size_dist': {\"1\":{\"sum\":0, \"count\":0}, \"2\":{\"sum\":0, \"count\":0},\n",
    "                                            \"3\":{\"sum\":0, \"count\":0},\"4\":{\"sum\":0, \"count\":0}},\n",
    "                    'process_sharing': set(),\n",
    "                    'ds_size_dist': {\"sum\":0, \"count\":0},\n",
    "                }\n",
    "            \n",
    "            \n",
    "            find = h5d_df_fc[h5d_df_fc['id'] == ds_id].index[0]\n",
    "            dataset_intents.session_io.open_timestamp = (h5d_df_fc['H5D_F_OPEN_START_TIMESTAMP'][find],\n",
    "                                                        h5d_df_fc['H5D_F_OPEN_END_TIMESTAMP'][find])\n",
    "            dataset_intents.session_io.close_timestamp = (h5d_df_fc['H5D_F_CLOSE_START_TIMESTAMP'][find],\n",
    "                                                        h5d_df_fc['H5D_F_CLOSE_END_TIMESTAMP'][find])\n",
    "            dataset_intents.session_io.read_timestamp = (h5d_df_fc['H5D_F_READ_START_TIMESTAMP'][find],\n",
    "                                                        h5d_df_fc['H5D_F_READ_END_TIMESTAMP'][find])\n",
    "            dataset_intents.session_io.write_timestamp = (h5d_df_fc['H5D_F_WRITE_START_TIMESTAMP'][find],\n",
    "                                                        h5d_df_fc['H5D_F_WRITE_END_TIMESTAMP'][find])\n",
    "            dtype_size = h5d_df_c['H5D_DATATYPE_SIZE'][ind]\n",
    "            num_elements_written = h5d_df_c['H5D_BYTES_WRITTEN'][ind] / dtype_size\n",
    "            num_elements_read = h5d_df_c['H5D_BYTES_READ'][ind] / dtype_size\n",
    "            \n",
    "            if num_elements_written == 0 and num_elements_read > 0:\n",
    "                dataset_intents.mode = FileMode.READ_ONLY\n",
    "                dataset_intents.type = AccessPatternType.READ_ONLY\n",
    "                file_agg[file_id]['ap_distribution'][str(AccessPatternType.READ_ONLY.value)] += 1\n",
    "                file_agg[file_id]['mode'][str(FileMode.READ_ONLY.value)] += 1\n",
    "            elif num_elements_written > 0 and num_elements_read == 0:\n",
    "                dataset_intents.mode = FileMode.WRITE_ONLY\n",
    "                dataset_intents.type = AccessPatternType.WRITE_ONLY\n",
    "                file_agg[file_id]['ap_distribution'][str(AccessPatternType.WRITE_ONLY.value)] += 1\n",
    "                file_agg[file_id]['mode'][str(FileMode.WRITE_ONLY.value)] += 1\n",
    "            else:\n",
    "                dataset_intents.mode = FileMode.READ_WRITE\n",
    "                file_agg[file_id]['mode'][str(FileMode.READ_WRITE.value)] += 1\n",
    "                if h5d_df_fc['H5D_F_READ_START_TIMESTAMP'][find] > h5d_df_fc['H5D_F_WRITE_END_TIMESTAMP'][find]:\n",
    "                    dataset_intents.type = AccessPatternType.RAW\n",
    "                    file_agg[file_id]['ap_distribution'][str(AccessPatternType.RAW.value)] += 1\n",
    "                else:\n",
    "                    dataset_intents.type = AccessPatternType.OTHER\n",
    "                    file_agg[file_id]['ap_distribution'][str(AccessPatternType.OTHER.value)] += 1\n",
    "            dataset_intents.ndims = h5d_df_c['H5D_DATASPACE_NDIMS'][ind]\n",
    "            dataset_intents.top_accessed_segments = {}\n",
    "            dataset_intents.transfer_size_dist = {}\n",
    "            for i in range(1,4):\n",
    "                dataset_intents.top_accessed_segments[str(i)] = {\"length\": [],\n",
    "                                           \"count\": h5d_df_c[f'H5D_ACCESS{i}_COUNT'][ind],\n",
    "                                           \"stride\": [],\n",
    "                                           \"access\": h5d_df_c[f'H5D_ACCESS{i}_ACCESS'][ind]}\n",
    "                dataset_intents.transfer_size_dist[str(i)] = 1\n",
    "                for dim_ind in range(0, dataset_intents.ndims):\n",
    "                    end_dim = 5 - dim_ind\n",
    "                    dataset_intents.top_accessed_segments[str(i)][\"length\"].append(h5d_df_c[f'H5D_ACCESS{i}_LENGTH_D{end_dim}'][ind])\n",
    "                    dataset_intents.top_accessed_segments[str(i)][\"stride\"].append(h5d_df_c[f'H5D_ACCESS{i}_STRIDE_D{end_dim}'][ind])\n",
    "                    dataset_intents.transfer_size_dist[str(i)] *= h5d_df_c[f'H5D_ACCESS{i}_LENGTH_D{end_dim}'][ind]\n",
    "                file_agg[file_id]['transfer_size_dist'][str(i)][\"sum\"] += dataset_intents.transfer_size_dist[str(i)]\n",
    "                file_agg[file_id]['transfer_size_dist'][str(i)][\"count\"] += 1\n",
    "                \n",
    "            dataset_intents.process_sharing = [h5d_df_c['rank'][ind]]\n",
    "            dataset_intents.sharing_pattern = SharingPattern.INDEPENDENT\n",
    "            if h5d_df_c['rank'][ind] == -1:\n",
    "                dataset_intents.sharing_pattern = SharingPattern.COLLECTIVE\n",
    "                dataset_intents.process_sharing = list(range(self.app[app_name]['num_processes']))\n",
    "            file_agg[file_id]['process_sharing'].update(dataset_intents.process_sharing)\n",
    "            dataset_intents.fs_size = h5d_df_c['H5D_BYTES_WRITTEN'][ind] \\\n",
    "                                        if h5d_df_c['H5D_BYTES_WRITTEN'][ind] > h5d_df_c['H5D_BYTES_READ'][ind] \\\n",
    "                                        else h5d_df_c['H5D_BYTES_READ'][ind]\n",
    "            file_agg[file_id][\"fs_size\"] += dataset_intents.fs_size\n",
    "            file_agg[file_id][\"ds_size_dist\"][\"sum\"] += dataset_intents.fs_size\n",
    "            file_agg[file_id][\"ds_size_dist\"][\"count\"] += 1\n",
    "            self.app[app_name]['configuration'].datasets[dataset_intents.dataset_name] = dataset_intents\n",
    "            #print(dataset_intents)\n",
    "        self.app[app_name]['file_agg'] = file_agg\n",
    "        return self.app[app_name]['configuration'], self.app[app_name]['file_agg']\n",
    "            \n",
    "    def read_file(self, app_name):\n",
    "        h5f_df_c = self.app[app_name]['h5f_df_c']\n",
    "        h5d_df_c = self.app[app_name]['h5d_df_c']\n",
    "        h5f_df_fc = self.app[app_name]['h5f_df_fc']\n",
    "        h5d_df_fc = self.app[app_name]['h5d_df_fc']\n",
    "        report = self.app[app_name]['report']\n",
    "        file_agg = self.app[app_name]['file_agg']\n",
    "        for ind in h5f_df_c.index:\n",
    "            file_id = h5f_df_c['id'][ind]\n",
    "            file_agg_item = file_agg[file_id]\n",
    "            find = h5f_df_fc[h5f_df_fc['id'] == file_id].index[0]\n",
    "            file_item = FileIOIntents()\n",
    "            file_item.filename = report.data['name_records'][file_id]\n",
    "            file_item.session_io.open_timestamp = (h5f_df_fc['H5F_F_OPEN_START_TIMESTAMP'][find],\n",
    "                                                    h5f_df_fc['H5F_F_OPEN_END_TIMESTAMP'][find])\n",
    "            file_item.session_io.close_timestamp = (h5f_df_fc['H5F_F_CLOSE_START_TIMESTAMP'][find],\n",
    "                                                    h5f_df_fc['H5F_F_CLOSE_END_TIMESTAMP'][find])\n",
    "            if file_agg_item[\"mode\"][str(FileMode.READ_ONLY.value)] == 0:\n",
    "                file_item.mode = FileMode.WRITE_ONLY\n",
    "            elif file_agg_item[\"mode\"][str(FileMode.WRITE_ONLY.value)] == 0:\n",
    "                file_item.mode = FileMode.READ_ONLY\n",
    "            else:\n",
    "                file_item.mode = FileMode.READ_WRITE\n",
    "            file_item.fs_size = file_agg_item[\"fs_size\"]\n",
    "            file_item.process_sharing = list(file_agg_item[\"process_sharing\"])\n",
    "            file_item.sharing_pattern = SharingPattern.INDEPENDENT\n",
    "            if len(file_item.process_sharing) > 1:\n",
    "                file_item.sharing_pattern = SharingPattern.COLLECTIVE\n",
    "            file_item.ap_distribution = file_agg_item[\"ap_distribution\"]\n",
    "            file_item.top_accessed_segments = file_agg_item[\"top_accessed_segments\"]\n",
    "            file_item.transfer_size_dist = file_agg_item[\"transfer_size_dist\"]\n",
    "            file_item.ds_size_dist = file_agg_item[\"ds_size_dist\"]\n",
    "            self.app[app_name]['configuration'].files[file_item.filename] = file_item\n",
    "        return self.app[app_name]['configuration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd1fa765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to intent_generator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a intent_generator.py\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Generate H5Bench Config')\n",
    "    parser.add_argument(\"--base-path\", default=\"\", type=str,\n",
    "                        help=\"Base path where darshan logs are present.\")\n",
    "    parser.add_argument(\"--darshan-logs\", default=\"\", type=str, help=\"Darshan log dir relative to base path\")\n",
    "    parser.add_argument(\"--property-json\", default=\"\", type=str, help=\"Property json dir relative to base path\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    folder = f\"{args.base_path}/{args.darshan_logs}\"\n",
    "    for workflow in os.listdir(folder):\n",
    "        print(f\"Generating config for workflow {workflow}\")\n",
    "        generator = IntentGenerator(args.base_path, args.darshan_logs, args.property_json, workflow)\n",
    "        generator.parse_apps()\n",
    "        generator.load_apps()\n",
    "        generator.write_configurations()\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9b81577b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating config for workflow sync-write-1d-strided-small in /usr/workspace/iopp/software/h5intent/logs/darshan/sync_libdarshan_none_1_40\n",
      "dict_keys(['POSIX', 'H5F', 'H5D', 'STDIO', 'HEATMAP'])\n",
      "Generating config for workflow sync-write-1d-strided-medium-col in /usr/workspace/iopp/software/h5intent/logs/darshan/sync_libdarshan_none_1_40\n",
      "dict_keys(['POSIX', 'MPI-IO', 'H5F', 'H5D', 'STDIO', 'HEATMAP'])\n",
      "Generating config for workflow sync-write-1d-strided-large-col in /usr/workspace/iopp/software/h5intent/logs/darshan/sync_libdarshan_none_1_40\n",
      "dict_keys(['POSIX', 'MPI-IO', 'H5F', 'H5D', 'STDIO', 'HEATMAP'])\n",
      "Generating config for workflow sync-write-1d-strided-large in /usr/workspace/iopp/software/h5intent/logs/darshan/sync_libdarshan_none_1_40\n",
      "dict_keys(['POSIX', 'H5F', 'H5D', 'STDIO', 'HEATMAP'])\n",
      "Generating config for workflow sync-write-1d-strided-medium in /usr/workspace/iopp/software/h5intent/logs/darshan/sync_libdarshan_none_1_40\n",
      "dict_keys(['POSIX', 'H5F', 'H5D', 'STDIO', 'HEATMAP'])\n",
      "Generating config for workflow sync-write-1d-strided-small-col in /usr/workspace/iopp/software/h5intent/logs/darshan/sync_libdarshan_none_1_40\n",
      "dict_keys(['POSIX', 'MPI-IO', 'H5F', 'H5D', 'STDIO', 'HEATMAP'])\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/usr/workspace/iopp/software/h5intent\"\n",
    "darshan_logs = \"logs/darshan/sync_libdarshan_none_1_40\"\n",
    "property_json = \"logs/property-json/sync_libdarshan_none_1_40\"\n",
    "folder = f\"{base_path}/{darshan_logs}\"\n",
    "data_dirs = \"/p/gpfs1/\"\n",
    "for workflow in os.listdir(folder):\n",
    "    print(f\"Generating config for workflow {workflow} in {folder}\")\n",
    "    generator = IntentGenerator(base_path, darshan_logs, property_json, workflow, data_dirs)\n",
    "    generator.parse_apps()\n",
    "    generator.load_apps()\n",
    "    generator.write_configurations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0eb43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50a774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc229842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
